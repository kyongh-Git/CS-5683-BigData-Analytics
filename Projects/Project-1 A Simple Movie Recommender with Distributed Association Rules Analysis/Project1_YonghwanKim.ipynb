{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jCgNF449zEk"
      },
      "source": [
        "Yonghwan Kim A11746276"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98ikFsksTvD",
        "outputId": "31b3d9aa-d224-4deb-a481-01ed19bc13dd"
      },
      "source": [
        "# pyspark install\n",
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 76 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 68.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=3e2016d8c3f786368794b25b25da244d0724ae40880be958b390c15b2f42d1d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fo67rIK0v4_",
        "outputId": "9497f54f-35b4-4e4d-e294-05885fcc1852"
      },
      "source": [
        "import sys\n",
        "from itertools import chain, combinations # iteration tool\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz8GjvGm0v0F"
      },
      "source": [
        "# create Spark context with necessary configuration\n",
        "sc = SparkContext(\"local\",\"PySpark - CS5683 - Project-1\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX5bYfEO6zGw"
      },
      "source": [
        "views = '/content/drive/MyDrive/views.txt'\n",
        "get_recommendations = '/content/drive/MyDrive/get_recommendations.txt'\n",
        "mappings = '/content/drive/MyDrive/mappings.txt'\n",
        "\n",
        "# support threshold\n",
        "s = 3\n",
        "# Iteration\n",
        "k = 2\n",
        "# confidence\n",
        "c = 0.6\n",
        "\n",
        "# remove unecessary data\n",
        "def associationRule(x):\n",
        "    x[0].remove(x[1])\n",
        "    return x\n",
        "# find all the subsets of basket with given range\n",
        "def powerset(iterable, n):\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(n, n+1))\n",
        "# changing to tuple to digest iterable type\n",
        "def distribute(iterable):\n",
        "    return tuple(iterable)\n",
        "# make candidate combinations with sigleton broadcast and k-th combinations\n",
        "def comb(x):\n",
        "    combined = []\n",
        "    for single in singleBroadcast.value:        \n",
        "          if not set(single).issubset(set(x)):\n",
        "             combined.append(set(single).union(set(x)))\n",
        "    return combined\n",
        "\n",
        "# launch view.txt to spark\n",
        "fileRDD = sc.textFile(views)\n",
        "wordsRDD = fileRDD.flatMap(lambda line: line.split(\"\\n\"))\n",
        "# create singleton with it's support value\n",
        "itemsRDD = wordsRDD.flatMap(lambda line: line.split(\"::\"))\n",
        "singletonRDD = itemsRDD.map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= s)\n",
        "\n",
        "# broadcast singleton\n",
        "singleBroadcast = sc.broadcast(singletonRDD.keys().map(lambda i:tuple((i,))).collect())\n",
        "# create basket rdd\n",
        "basketRDD = wordsRDD.map(lambda line: line.split(\"::\")).map(lambda word: frozenset(word))\n",
        "# initialize frequent itemsets rdd empty\n",
        "frequentItemsetRDD = sc.emptyRDD()\n",
        "# intialize candidate itemsets rdd with singleton keys\n",
        "candidateItemsetRDD = singletonRDD.keys()\n",
        "\n",
        "# for loop to create candidate itemsets and its support value to filter out later.\n",
        "while k <= 4:      \n",
        "      # create candidates using broadcasted singleton and k-th list.\n",
        "      if k == 2:\n",
        "          candidateRDD = candidateItemsetRDD.flatMap(lambda a : comb((a,))).map(lambda item: (frozenset(item) , 1)).reduceByKey(lambda a,b: a+b)\n",
        "      else:\n",
        "          candidateRDD = candidateItemsetRDD.flatMap(lambda a : comb(a)).map(lambda item: (frozenset(item) , 1)).reduceByKey(lambda a,b: a+b)\n",
        "      # find support value of k-th list from basket\n",
        "      scountbasketRDD = basketRDD.flatMap(lambda word: distribute(powerset(word, k))).map(lambda w: (frozenset(w), 1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1] >= s)\n",
        "      # find matching candidates\n",
        "      frequentCandidateRDD = candidateRDD.join(scountbasketRDD).map(lambda item: (item[0], item[1][1]))\n",
        "      # combine current k itemsets with previous list\n",
        "      frequentItemsetRDD = frequentItemsetRDD.union(frequentCandidateRDD)\n",
        "      # prepare k itemsets for next iteration step\n",
        "      candidateItemsetRDD = frequentCandidateRDD.keys()\n",
        "      # increase k to go next iteration\n",
        "      k = k + 1"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFhCkaYIS9UQ"
      },
      "source": [
        "# generate association rules\n",
        "associationRDD = frequentItemsetRDD.cartesian(singletonRDD).filter(lambda i : any(i[1][0] == x for x in i[0][0]))\n",
        "# calculate confidence level and filter the rules that are lesser than confidence threshold 60%\n",
        "confiRDD = associationRDD.map(lambda i : (i, (i[0][1]/i[1][1]))).filter(lambda i : i[1] >= c)\n",
        "# sort all association \n",
        "confiRDD = confiRDD.sortBy(lambda x: x[1], False)\n",
        "\n",
        "finalAssociationRDD = confiRDD.map(lambda i : (list(i[0][0][0]), i[0][1][0], i[1]*100)).map(associationRule)\n",
        "# save the association rules in one output file\n",
        "finalAssociationRDD.coalesce(1,True).saveAsTextFile(\"/content/association\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKHejLn7gEin",
        "outputId": "a2e620ec-5945-44e8-8b22-8516f379285e"
      },
      "source": [
        "#Bonus Point\n",
        "recomRDD = sc.textFile(get_recommendations).flatMap(lambda line: line.split(\"\\n\")).map(lambda line: line.split(\"::\"))\n",
        "recommBroadcast = sc.broadcast(recomRDD.collect())\n",
        "associBroadcast = sc.broadcast(finalAssociationRDD.collect())\n",
        "\n",
        "# find 3 movies for each watch list\n",
        "temp = []\n",
        "\n",
        "for rec in recommBroadcast.value:\n",
        "    # print(set(rec))\n",
        "    recommd = 0\n",
        "    rectemp = []\n",
        "    # broadcasted association rules\n",
        "    for a in associBroadcast.value:\n",
        "          if recommd == 3:\n",
        "            break\n",
        "          if set(a[0]).issubset(set(rec)):\n",
        "              # recommand 3 movies that are not already recommanded and not already watched\n",
        "              if not set([a[1]]).issubset(set(rec)) and not set([a[1]]).issubset(set(rectemp)): \n",
        "                  rectemp.append(a[1])\n",
        "                  recommd = recommd + 1\n",
        "    temp.append(rectemp)\n",
        "# print(temp)\n",
        "# find actual name of the moives\n",
        "mappingRDD = sc.textFile(mappings).flatMap(lambda line: line.split(\"\\n\")).map(lambda line: line.split(\"::\"))\n",
        "mappingBroadcast = sc.broadcast(mappingRDD.collect())\n",
        "\n",
        "title = []\n",
        "for t in temp:\n",
        "    indTitle = []\n",
        "    for l in t:\n",
        "        for m in mappingBroadcast.value:\n",
        "            if l == m[0]:\n",
        "                indTitle.append(m[1])\n",
        "    title.append(indTitle)\n",
        "# final result\n",
        "print(title)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[], [], [], ['Rocky (1976)', 'Play it to the Bone (1999)', 'Grand Day Out, A (1992)'], ['Exorcist II: The Heretic (1977)', 'Secret of NIMH, The (1982)', 'eXistenZ (1999)'], ['Hate (Haine, La) (1995)', 'Cutthroat Island (1995)'], ['Nixon (1995)', 'Suicide Kings (1997)', 'Rocky (1976)'], ['Better Living Through Circuitry (1999)', 'Gnome-Mobile, The (1967)', \"Nightmare on Elm Street Part 2: Freddy's Revenge, A (1985)\"], [], ['Tough Guys (1986)', 'Rough Magic (1995)', 'Zeus and Roxanne (1997)']]\n"
          ]
        }
      ]
    }
  ]
}