{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Assignment-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ844o3Oo_y9"
      },
      "source": [
        "# CS 5683 - Big Data Analytics\n",
        "## Assignment - 1: Intro. to Spark and RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Xvw-rio_y_"
      },
      "source": [
        "###### Use Google Colab to use this notebook\n",
        "###### Let's setup Spark first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUIRsN79o_zA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14adcbc9-1745-4056-e424-fb7f78e671a7"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 64 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=bdc8e8b6637fa769027f953b2702653702a6860f3e55cd127ccfa3e6eced40a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXjnu07mo_zC"
      },
      "source": [
        "###### Import required libraries now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzZN0SjIo_zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05b397f-d34e-4a4e-acb3-d3defd3a092b"
      },
      "source": [
        "import sys\n",
        "import re\n",
        " \n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcK6bASio_zD"
      },
      "source": [
        "###### Let's initialize Spark context now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTGUvTySo_zE"
      },
      "source": [
        "# create Spark context with necessary configuration\n",
        "sc = SparkContext(\"local\",\"PySpark - CS5683 - Assignment-1\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkvlhCDCo_zE"
      },
      "source": [
        "###### Follow the tutorial to mount your Google Drive. Give mounted Drive paths below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNN62IhVo_zF"
      },
      "source": [
        "# Give **.txt FILE PATHS** here\n",
        "# Use your own files which could match your application\n",
        "file1 = '/content/drive/MyDrive/file1.txt'\n",
        "file2 = '/content/drive/MyDrive/file2.txt'\n",
        "\n",
        "# USE THESE FILES as input(s) FOR ALL BELOW QUESTIONS"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cQDb9z9o_zH"
      },
      "source": [
        "### Example Spark program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ysC-Ujxo_zH"
      },
      "source": [
        "# Example Spark application for a simple wordcount\n",
        "# What is wordcount? \n",
        "    # Given a file, count the frequency of all words appearing in that file\n",
        "    \n",
        "# Step-1: Read the required file. In our case it is file1 or file2.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD = sc.textFile(file1)\n",
        "\n",
        "# Step-2: \n",
        "    # Each line in our file(s) is a sentence. So, we need to split the sentence with ' ' to get words\n",
        "    # Using map() will return RDD[list]. But we need RDD[string]. So we use flatMap()\n",
        "wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n",
        "\n",
        "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDD = wordsRDD.map(lambda word: (word,1))\n",
        "pairRDD.collect()\n",
        "# Step-4: Now we have to sum all 1's of each word\n",
        "# NOTE: A word may present in multiple data partitions. So we use reduceByKey() to group by key and perform sum\n",
        "# countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#Step-5: Save results in a text file\n",
        "# countRDD.saveAsTextFile('/content/test') # <----------- GIVE FILE PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6LHqn7Ro_zI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92KMLdugo_zJ"
      },
      "source": [
        "### Question - 1 (10 points) "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "PGzcYDMxo_zJ"
      },
      "source": [
        "Write a Spark application that counts number of non-unique words that begins with each special character in file1. That is count the number of words (including duplicates) that begins with each special characters. You can **ignore** letter cases (consider the given text contains only lower-case letters). \n",
        "\n",
        "Example Output:\n",
        "('#', 500)\n",
        "('[', 100)\n",
        "\n",
        "which means that the given input file has 500 words that begin with the special character '#' and 100 words that begin with the special character '['.\n",
        "NOTE: The application counts duplicate words also and we need to use Python's regex to find special characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXgx5Qvdo_zK"
      },
      "source": [
        "# YOUR CODE for Question-1 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svm473gwo_zK"
      },
      "source": [
        "# Question 1\n",
        "# Step-1: Read the file1.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD = sc.textFile(file1)\n",
        "\n",
        "# Step-2: Split the sentence with ' ' to get words RDD[string] using flatMap() \n",
        "wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Step-3: Take first letter of each words.\n",
        "lettersRDD = wordsRDD.map(lambda line: line[:1])\n",
        "\n",
        "# Step-4: Take out emoticons from the list\n",
        "EMOJI_Kill = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])') \n",
        "noemojiRDD = lettersRDD.flatMap(lambda l: EMOJI_Kill.sub(r'', l))\n",
        "\n",
        "# Step-5: Leave only special characters.\n",
        "filteredRDD = noemojiRDD.flatMap(lambda l: re.findall(\"\\W\" ,l)).filter(lambda x: x)\n",
        "\n",
        "# Step-6: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDD = filteredRDD.map(lambda word: (word,1))\n",
        "\n",
        "# Step-7: Use reduceByKey() to group by key and perform sum\n",
        "countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n",
        "#countRDD.collect()\n",
        "# Step-8: Save results in a text file\n",
        "countRDD.saveAsTextFile('/content/test1') # <----------- GIVE FILE PATH"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piz95XHUo_zK"
      },
      "source": [
        "# PRINT THE OUTPUT HERE\n",
        "[('#', 9), ('@', 5), ('!', 2), ('[', 1), (']', 1), ('\"', 4)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-shNeVo_zL"
      },
      "source": [
        "### Question - 2 (10 points)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "G2Mg9Xgao_zL"
      },
      "source": [
        "Write a Spark application that counts the number of unique words in each line  in file1. You **do not** need to remove any non-alphabetic letters and special characters for this application. Similarly, you can **ignore** letter cases (consider the given text contains only lower-case letters)\n",
        "\n",
        "Example Output:\n",
        "(1, 100)\n",
        "(2, 700)\n",
        "(3, 1500)\n",
        "\n",
        "which means that the input file for the Spark application has 100 unique words in line 1, 700 unique words in line 2 , and 1500 uniqe words in line 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm6jNXLro_zL"
      },
      "source": [
        "# YOUR CODE for Question-2 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkeLZnLo_zL"
      },
      "source": [
        "# Question 2\n",
        "# Step-1: Read the file1.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD = sc.textFile(file1)\n",
        "\n",
        "# Step-2: Split the line with '\\n' to get lines RDD[sentence] using flatMap() \n",
        "linesplitRDD = fileRDD.flatMap(lambda line: line.split(\"\\n\")).filter(lambda x: x)\n",
        "\n",
        "# Step-3: Split the sentence with ' ' to get words [[words], [words]]\n",
        "spacesplitRDD = linesplitRDD.map(lambda word: word.lower().split(\" \"))\n",
        "\n",
        "# Step-4: Change to set datastructure to remove duplicated words\n",
        "setRDD = spacesplitRDD.map(lambda word: set(word))\n",
        "\n",
        "# Step-5: Count each unique words in each line\n",
        "distRDD = setRDD.map(lambda word: len(word))\n",
        "\n",
        "# Step-6: Indexing the previous rdd\n",
        "indexRDD = distRDD.zipWithIndex().map(lambda x : (x[1]+1,x[0]))\n",
        "\n",
        "# indexRDD.collect()\n",
        "# Step-8: Save results in a text file\n",
        "indexRDD.saveAsTextFile('/content/test2') # <----------- GIVE FILE PATH"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-4-Irjro_zL"
      },
      "source": [
        "# PRINT THE OUTPUT HERE\n",
        "\n",
        "[(1, 5),\n",
        " (2, 3),\n",
        " (3, 10),\n",
        " (4, 4),\n",
        " (5, 4),\n",
        " (6, 4),\n",
        " (7, 4),\n",
        " (8, 4),\n",
        " (9, 4),\n",
        " (10, 4),\n",
        " (11, 4),\n",
        " (12, 4),\n",
        " (13, 4),\n",
        " (14, 4),\n",
        " (15, 4),\n",
        " (16, 4),\n",
        " (17, 4),\n",
        " (18, 4),\n",
        " (19, 4),\n",
        " (20, 4),\n",
        " (21, 4),\n",
        " (22, 4),\n",
        " (23, 4),\n",
        " (24, 4),\n",
        " (25, 4),\n",
        " (26, 4),\n",
        " (27, 4),\n",
        " (28, 4),\n",
        " (29, 4),\n",
        " (30, 4),\n",
        " (31, 4),\n",
        " (32, 4),\n",
        " (33, 4),\n",
        " (34, 4),\n",
        " (35, 4),\n",
        " (36, 4),\n",
        " (37, 4),\n",
        " (38, 4),\n",
        " (39, 4),\n",
        " (40, 4),\n",
        " (41, 4),\n",
        " (42, 4),\n",
        " (43, 4),\n",
        " (44, 4),\n",
        " (45, 4),\n",
        " (46, 4),\n",
        " (47, 4),\n",
        " (48, 4),\n",
        " (49, 4),\n",
        " (50, 4),\n",
        " (51, 4),\n",
        " (52, 4),\n",
        " (53, 4),\n",
        " (54, 4),\n",
        " (55, 4),\n",
        " (56, 4),\n",
        " (57, 4),\n",
        " (58, 4),\n",
        " (59, 4),\n",
        " (60, 4),\n",
        " (61, 4),\n",
        " (62, 4),\n",
        " (63, 4),\n",
        " (64, 4),\n",
        " (65, 4),\n",
        " (66, 4),\n",
        " (67, 4),\n",
        " (68, 4),\n",
        " (69, 4),\n",
        " (70, 4),\n",
        " (71, 4),\n",
        " (72, 4),\n",
        " (73, 4),\n",
        " (74, 4),\n",
        " (75, 4),\n",
        " (76, 4),\n",
        " (77, 4),\n",
        " (78, 4),\n",
        " (79, 4),\n",
        " (80, 4),\n",
        " (81, 4),\n",
        " (82, 4),\n",
        " (83, 4),\n",
        " (84, 4),\n",
        " (85, 4),\n",
        " (86, 4),\n",
        " (87, 4),\n",
        " (88, 4),\n",
        " (89, 4),\n",
        " (90, 3),\n",
        " (91, 4)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCWDg-_Oo_zL"
      },
      "source": [
        "### Question - 3 (10 points)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "fBSdoL84o_zL"
      },
      "source": [
        "Write a Spark aplication that outputs wordcount from two files: file1 and file2. That is count the number of occurances of words from both file1 and file2. You can **ignore** letter cases (consider the given input files contain only lower-case letters). Also, you can **ignore** words that is not present in both files. **Sort the output in alphabetical order**\n",
        "\n",
        "Example Output:\n",
        "(big, (10, 20))\n",
        "(Data, (30, 50))\n",
        "\n",
        "which means the word \"big\" appears 10 times in file-1 and 20 times in file-2 and the word \"Data\" appears 30 times in file-1 and 50 times in file-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGPyOq6o_zM"
      },
      "source": [
        "# YOUR CODE for Question-3 HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQErMFKno_zM"
      },
      "source": [
        "# Question 3\n",
        "# Step-1: Read the file1 and file2.\n",
        "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
        "fileRDD1 = sc.textFile(file1)\n",
        "fileRDD2 = sc.textFile(file2)\n",
        "\n",
        "# Step-2: Split the sentence with ' ' to get words RDD[string] using flatMap() \n",
        "wordsRDD1 = fileRDD1.flatMap(lambda line: line.split(\" \")).filter(lambda x: x)\n",
        "wordsRDD2 = fileRDD2.flatMap(lambda line: line.split(\" \")).filter(lambda x: x)\n",
        "\n",
        "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
        "pairRDD1 = wordsRDD1.map(lambda word: (word.lower(), 1))\n",
        "pairRDD2 = wordsRDD2.map(lambda word: (word.lower(), 1))\n",
        "\n",
        "# Step-4: Use reduceByKey() to group by key and perform sum\n",
        "countRDD1 = pairRDD1.reduceByKey(lambda a,b: a+b)\n",
        "countRDD2 = pairRDD2.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "# Step-5: Join two rdds and sort them in ascending order by key\n",
        "sortRDD = countRDD1.join(countRDD2).sortByKey()\n",
        "\n",
        "# sortRDD.collect()\n",
        "# Step-8: Save results in a text file\n",
        "sortRDD.saveAsTextFile('/content/test3') # <----------- GIVE FILE PATH"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s3wLBRro_zM"
      },
      "source": [
        "# PRINT THE OUTPUT HERE\n",
        "[('!d', (1, 1)),\n",
        " ('!test', (1, 1)),\n",
        " ('#', (1, 1)),\n",
        " ('#test', (4, 4)),\n",
        " ('@', (1, 1)),\n",
        " ('@@', (1, 1)),\n",
        " ('@f', (1, 1)),\n",
        " ('@hard', (1, 1)),\n",
        " ('@life', (1, 1)),\n",
        " ('[', (1, 1)),\n",
        " (']', (1, 1)),\n",
        " ('abandon', (1, 2)),\n",
        " ('ability', (1, 2)),\n",
        " ('able', (1, 2)),\n",
        " ('abortion', (1, 2)),\n",
        " ('about', (1, 2)),\n",
        " ('above', (1, 2)),\n",
        " ('abroad', (1, 2)),\n",
        " ('absence', (1, 2)),\n",
        " ('absolute', (1, 2)),\n",
        " ('absolutely', (1, 2)),\n",
        " ('absorb', (1, 2)),\n",
        " ('abuse', (1, 2)),\n",
        " ('academic', (1, 2)),\n",
        " ('accept', (1, 2)),\n",
        " ('access', (1, 2)),\n",
        " ('accident', (1, 2)),\n",
        " ('accompany', (1, 2)),\n",
        " ('accomplish', (1, 2)),\n",
        " ('according', (1, 2)),\n",
        " ('account', (1, 2)),\n",
        " ('accurate', (1, 2)),\n",
        " ('accuse', (1, 2)),\n",
        " ('achieve', (1, 2)),\n",
        " ('achievement', (1, 2)),\n",
        " ('acid', (1, 2)),\n",
        " ('acknowledge', (1, 2)),\n",
        " ('acquire', (1, 2)),\n",
        " ('across', (1, 2)),\n",
        " ('act', (1, 2)),\n",
        " ('action', (1, 2)),\n",
        " ('active', (1, 2)),\n",
        " ('activist', (1, 2)),\n",
        " ('activity', (1, 2)),\n",
        " ('actor', (1, 2)),\n",
        " ('actress', (1, 2)),\n",
        " ('actual', (1, 2)),\n",
        " ('actually', (1, 2)),\n",
        " ('ad', (1, 2)),\n",
        " ('adapt', (1, 2)),\n",
        " ('add', (1, 2)),\n",
        " ('addition', (1, 2)),\n",
        " ('additional', (1, 2)),\n",
        " ('address', (1, 2)),\n",
        " ('adequate', (1, 2)),\n",
        " ('adjust', (1, 2)),\n",
        " ('adjustment', (1, 2)),\n",
        " ('administration', (1, 2)),\n",
        " ('administrator', (1, 2)),\n",
        " ('admire', (1, 2)),\n",
        " ('admission', (1, 2)),\n",
        " ('admit', (1, 2)),\n",
        " ('adolescent', (1, 2)),\n",
        " ('adopt', (1, 2)),\n",
        " ('adult', (1, 2)),\n",
        " ('advance', (1, 2)),\n",
        " ('advanced', (1, 2)),\n",
        " ('advantage', (1, 2)),\n",
        " ('adventure', (1, 2)),\n",
        " ('advertising', (1, 2)),\n",
        " ('advice', (1, 2)),\n",
        " ('advise', (1, 2)),\n",
        " ('adviser', (1, 2)),\n",
        " ('advocate', (1, 2)),\n",
        " ('affair', (1, 2)),\n",
        " ('affect', (1, 2)),\n",
        " ('afford', (1, 2)),\n",
        " ('afraid', (1, 2)),\n",
        " ('african', (1, 2)),\n",
        " ('african-american', (1, 2)),\n",
        " ('after', (1, 2)),\n",
        " ('afternoon', (1, 2)),\n",
        " ('again', (1, 2)),\n",
        " ('against', (1, 2)),\n",
        " ('age', (1, 2)),\n",
        " ('agency', (1, 2)),\n",
        " ('agenda', (1, 2)),\n",
        " ('agent', (1, 2)),\n",
        " ('aggressive', (1, 2)),\n",
        " ('ago', (1, 2)),\n",
        " ('agree', (1, 2)),\n",
        " ('agreement', (1, 2)),\n",
        " ('agricultural', (1, 2)),\n",
        " ('ah', (1, 2)),\n",
        " ('ahead', (1, 2)),\n",
        " ('aid', (1, 2)),\n",
        " ('aide', (1, 2)),\n",
        " ('amazing', (1, 1)),\n",
        " ('american', (1, 1)),\n",
        " ('among', (1, 1)),\n",
        " ('amount', (1, 1)),\n",
        " ('analysis', (1, 1)),\n",
        " ('analyst', (1, 1)),\n",
        " ('analyze', (1, 1)),\n",
        " ('ancient', (1, 1)),\n",
        " ('and', (1, 1)),\n",
        " ('anger', (1, 1)),\n",
        " ('angle', (1, 1)),\n",
        " ('angry', (1, 1)),\n",
        " ('animal', (1, 1)),\n",
        " ('anniversary', (1, 1)),\n",
        " ('announce', (1, 1)),\n",
        " ('annual', (1, 1)),\n",
        " ('another', (1, 1)),\n",
        " ('answer', (1, 1)),\n",
        " ('anticipate', (1, 1)),\n",
        " ('anxiety', (1, 1)),\n",
        " ('any', (1, 1)),\n",
        " ('anybody', (1, 1)),\n",
        " ('anymore', (1, 1)),\n",
        " ('anyone', (1, 1)),\n",
        " ('anything', (1, 1)),\n",
        " ('anyway', (1, 1)),\n",
        " ('anywhere', (1, 1)),\n",
        " ('apart', (1, 1)),\n",
        " ('apartment', (1, 1)),\n",
        " ('apparent', (1, 1)),\n",
        " ('apparently', (1, 1)),\n",
        " ('appeal', (1, 1)),\n",
        " ('appear', (1, 1)),\n",
        " ('appearance', (1, 1)),\n",
        " ('apple', (1, 1)),\n",
        " ('application', (1, 1)),\n",
        " ('apply', (1, 1)),\n",
        " ('appoint', (1, 1)),\n",
        " ('appointment', (1, 1)),\n",
        " ('appreciate', (1, 1)),\n",
        " ('approach', (1, 1)),\n",
        " ('appropriate', (1, 1)),\n",
        " ('approval', (1, 1)),\n",
        " ('approve', (1, 1)),\n",
        " ('approximately', (1, 1)),\n",
        " ('arab', (1, 1)),\n",
        " ('architect', (1, 1)),\n",
        " ('area', (1, 1)),\n",
        " ('argue', (1, 1)),\n",
        " ('argument', (1, 1)),\n",
        " ('arise', (1, 1)),\n",
        " ('arm', (1, 1)),\n",
        " ('armed', (1, 1)),\n",
        " ('army', (1, 1)),\n",
        " ('around', (1, 1)),\n",
        " ('arrange', (1, 1)),\n",
        " ('arrangement', (1, 1)),\n",
        " ('arrest', (1, 1)),\n",
        " ('arrival', (1, 1)),\n",
        " ('arrive', (1, 1)),\n",
        " ('art', (1, 1)),\n",
        " ('article', (2, 2)),\n",
        " ('artist', (1, 1)),\n",
        " ('artistic', (1, 1)),\n",
        " ('asian', (2, 1)),\n",
        " ('aside', (2, 1)),\n",
        " ('ask', (2, 1)),\n",
        " ('asleep', (2, 1)),\n",
        " ('aspect', (2, 1)),\n",
        " ('assault', (2, 1)),\n",
        " ('assert', (2, 1)),\n",
        " ('assess', (2, 1)),\n",
        " ('assessment', (2, 1)),\n",
        " ('asset', (2, 1)),\n",
        " ('assign', (2, 1)),\n",
        " ('assignment', (2, 1)),\n",
        " ('assist', (2, 1)),\n",
        " ('assistance', (2, 1)),\n",
        " ('assistant', (2, 1)),\n",
        " ('associate', (2, 1)),\n",
        " ('association', (2, 1)),\n",
        " ('assume', (2, 1)),\n",
        " ('assumption', (2, 1)),\n",
        " ('assure', (2, 1)),\n",
        " ('at', (2, 1)),\n",
        " ('athlete', (1, 1)),\n",
        " ('athletic', (1, 1)),\n",
        " ('atmosphere', (1, 1)),\n",
        " ('attach', (1, 1)),\n",
        " ('attack', (1, 1)),\n",
        " ('attempt', (1, 1)),\n",
        " ('attend', (1, 1)),\n",
        " ('attention', (1, 1)),\n",
        " ('attitude', (1, 1)),\n",
        " ('attorney', (1, 1)),\n",
        " ('attract', (1, 1)),\n",
        " ('attractive', (1, 1)),\n",
        " ('attribute', (1, 1)),\n",
        " ('audience', (1, 1)),\n",
        " ('author', (1, 1)),\n",
        " ('authority', (1, 1)),\n",
        " ('auto', (1, 1)),\n",
        " ('available', (1, 1)),\n",
        " ('average', (1, 1)),\n",
        " ('avoid', (1, 1)),\n",
        " ('award', (1, 1)),\n",
        " ('aware', (1, 1)),\n",
        " ('awareness', (1, 1)),\n",
        " ('away', (1, 1)),\n",
        " ('awful', (1, 1)),\n",
        " ('baby', (1, 1)),\n",
        " ('back', (1, 1)),\n",
        " ('background', (1, 1)),\n",
        " ('bad', (1, 1)),\n",
        " ('badly', (1, 1)),\n",
        " ('bag', (1, 1)),\n",
        " ('bake', (1, 1)),\n",
        " ('balance', (1, 1)),\n",
        " ('ball', (1, 1)),\n",
        " ('ban', (1, 1)),\n",
        " ('band', (1, 1)),\n",
        " ('bank', (1, 1)),\n",
        " ('bar', (1, 1)),\n",
        " ('barely', (1, 1)),\n",
        " ('barrel', (1, 1)),\n",
        " ('barrier', (1, 1)),\n",
        " ('base', (1, 1)),\n",
        " ('baseball', (1, 1)),\n",
        " ('basic', (1, 1)),\n",
        " ('basically', (1, 1)),\n",
        " ('basis', (1, 1)),\n",
        " ('basket', (1, 1)),\n",
        " ('basketball', (1, 1)),\n",
        " ('bathroom', (1, 1)),\n",
        " ('battery', (1, 1)),\n",
        " ('battle', (1, 1)),\n",
        " ('be', (1, 1)),\n",
        " ('beach', (1, 1)),\n",
        " ('bean', (1, 1)),\n",
        " ('bear', (1, 1)),\n",
        " ('beat', (1, 1)),\n",
        " ('beautiful', (1, 1)),\n",
        " ('beauty', (1, 1)),\n",
        " ('because', (1, 1)),\n",
        " ('become', (1, 1)),\n",
        " ('bed', (1, 1)),\n",
        " ('bedroom', (1, 1)),\n",
        " ('beer', (1, 1)),\n",
        " ('before', (1, 1)),\n",
        " ('s#df', (1, 1)),\n",
        " ('sdfs', (1, 1)),\n",
        " ('test!test', (1, 1)),\n",
        " ('wyed', (1, 1))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwyZ_Ako_zM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwG0Dvrxo_zN"
      },
      "source": [
        "### WHAT TO TURN-IN IN CANVAS"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "i8H4zB4Lo_zN"
      },
      "source": [
        "1. Complete questions 1,2, and 3\n",
        "2. Download this completed python notebook as .ipynb\n",
        "3. Upload it in Canvas assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOQjpTN-o_zO"
      },
      "source": [
        "# Due Date: Sept. 1 at 11:59pm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRt7O_bYo_zO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}